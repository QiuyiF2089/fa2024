{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DS 542 Notebook 6\n",
        "\n"
      ],
      "metadata": {
        "id": "0u0s4Ev-8oqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Data\n",
        "\n",
        "The following cells download the UCI Abalone data set.\n",
        "You should not need to modify any of these cells."
      ],
      "metadata": {
        "id": "SE706BcH8vA5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZmbh4jm7Jgd",
        "outputId": "3302fbb3-0476-4cb3-f118-7db213c8a00e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-28 03:53:49--  https://archive.ics.uci.edu/static/public/1/abalone.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘abalone.zip’\n",
            "\n",
            "abalone.zip             [  <=>               ]  54.06K   168KB/s    in 0.3s    \n",
            "\n",
            "2024-09-28 03:53:50 (168 KB/s) - ‘abalone.zip’ saved [55357]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://archive.ics.uci.edu/static/public/1/abalone.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip abalone.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnE9o9mz7Qlh",
        "outputId": "669c5c7d-18cc-4f7b-a726-ebdd74843980"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  abalone.zip\n",
            "  inflating: Index                   \n",
            "  inflating: abalone.data            \n",
            "  inflating: abalone.names           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat abalone.names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU97NROU7UNy",
        "outputId": "2cffabe0-d9fb-4f6d-ded9-f0f22ccdafb5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Title of Database: Abalone data\n",
            "\n",
            "2. Sources:\n",
            "\n",
            "   (a) Original owners of database:\n",
            "\tMarine Resources Division\n",
            "\tMarine Research Laboratories - Taroona\n",
            "\tDepartment of Primary Industry and Fisheries, Tasmania\n",
            "\tGPO Box 619F, Hobart, Tasmania 7001, Australia\n",
            "\t(contact: Warwick Nash +61 02 277277, wnash@dpi.tas.gov.au)\n",
            "\n",
            "   (b) Donor of database:\n",
            "\tSam Waugh (Sam.Waugh@cs.utas.edu.au)\n",
            "\tDepartment of Computer Science, University of Tasmania\n",
            "\tGPO Box 252C, Hobart, Tasmania 7001, Australia\n",
            "\n",
            "   (c) Date received: December 1995\n",
            "\n",
            "\n",
            "3. Past Usage:\n",
            "\n",
            "   Sam Waugh (1995) \"Extending and benchmarking Cascade-Correlation\", PhD\n",
            "   thesis, Computer Science Department, University of Tasmania.\n",
            "\n",
            "   -- Test set performance (final 1044 examples, first 3133 used for training):\n",
            "\t24.86% Cascade-Correlation (no hidden nodes)\n",
            "\t26.25% Cascade-Correlation (5 hidden nodes)\n",
            "\t21.5%  C4.5\n",
            "\t 0.0%  Linear Discriminate Analysis\n",
            "\t 3.57% k=5 Nearest Neighbour\n",
            "      (Problem encoded as a classification task)\n",
            "\n",
            "   -- Data set samples are highly overlapped.  Further information is required\n",
            "\tto separate completely using affine combinations.  Other restrictions\n",
            "\tto data set examined.\n",
            "\n",
            "   David Clark, Zoltan Schreter, Anthony Adams \"A Quantitative Comparison of\n",
            "   Dystal and Backpropagation\", submitted to the Australian Conference on\n",
            "   Neural Networks (ACNN'96). Data set treated as a 3-category classification\n",
            "   problem (grouping ring classes 1-8, 9 and 10, and 11 on).\n",
            "\n",
            "   -- Test set performance (3133 training, 1044 testing as above):\n",
            "\t64%    Backprop\n",
            "\t55%    Dystal\n",
            "   -- Previous work (Waugh, 1995) on same data set:\n",
            "\t61.40% Cascade-Correlation (no hidden nodes)\n",
            "\t65.61% Cascade-Correlation (5 hidden nodes)\n",
            "\t59.2%  C4.5\n",
            "\t32.57% Linear Discriminate Analysis\n",
            "\t62.46% k=5 Nearest Neighbour\n",
            "\n",
            "\n",
            "4. Relevant Information Paragraph:\n",
            "\n",
            "   Predicting the age of abalone from physical measurements.  The age of\n",
            "   abalone is determined by cutting the shell through the cone, staining it,\n",
            "   and counting the number of rings through a microscope -- a boring and\n",
            "   time-consuming task.  Other measurements, which are easier to obtain, are\n",
            "   used to predict the age.  Further information, such as weather patterns\n",
            "   and location (hence food availability) may be required to solve the problem.\n",
            "\n",
            "   From the original data examples with missing values were removed (the\n",
            "   majority having the predicted value missing), and the ranges of the\n",
            "   continuous values have been scaled for use with an ANN (by dividing by 200).\n",
            "\n",
            "   Data comes from an original (non-machine-learning) study:\n",
            "\n",
            "\tWarwick J Nash, Tracy L Sellers, Simon R Talbot, Andrew J Cawthorn and\n",
            "\tWes B Ford (1994) \"The Population Biology of Abalone (_Haliotis_\n",
            "\tspecies) in Tasmania. I. Blacklip Abalone (_H. rubra_) from the North\n",
            "\tCoast and Islands of Bass Strait\", Sea Fisheries Division, Technical\n",
            "\tReport No. 48 (ISSN 1034-3288)\n",
            "\n",
            "\n",
            "5. Number of Instances: 4177\n",
            "\n",
            "\n",
            "6. Number of Attributes: 8\n",
            "\n",
            "\n",
            "7. Attribute information:\n",
            "\n",
            "   Given is the attribute name, attribute type, the measurement unit and a\n",
            "   brief description.  The number of rings is the value to predict: either\n",
            "   as a continuous value or as a classification problem.\n",
            "\n",
            "\tName\t\tData Type\tMeas.\tDescription\n",
            "\t----\t\t---------\t-----\t-----------\n",
            "\tSex\t\tnominal\t\t\tM, F, and I (infant)\n",
            "\tLength\t\tcontinuous\tmm\tLongest shell measurement\n",
            "\tDiameter\tcontinuous\tmm\tperpendicular to length\n",
            "\tHeight\t\tcontinuous\tmm\twith meat in shell\n",
            "\tWhole weight\tcontinuous\tgrams\twhole abalone\n",
            "\tShucked weight\tcontinuous\tgrams\tweight of meat\n",
            "\tViscera weight\tcontinuous\tgrams\tgut weight (after bleeding)\n",
            "\tShell weight\tcontinuous\tgrams\tafter being dried\n",
            "\tRings\t\tinteger\t\t\t+1.5 gives the age in years\n",
            "\n",
            "   Statistics for numeric domains:\n",
            "\n",
            "\t\tLength\tDiam\tHeight\tWhole\tShucked\tViscera\tShell\tRings\n",
            "\tMin\t0.075\t0.055\t0.000\t0.002\t0.001\t0.001\t0.002\t    1\n",
            "\tMax\t0.815\t0.650\t1.130\t2.826\t1.488\t0.760\t1.005\t   29\n",
            "\tMean\t0.524\t0.408\t0.140\t0.829\t0.359\t0.181\t0.239\t9.934\n",
            "\tSD\t0.120\t0.099\t0.042\t0.490\t0.222\t0.110\t0.139\t3.224\n",
            "\tCorrel\t0.557\t0.575\t0.557\t0.540\t0.421\t0.504\t0.628\t  1.0\n",
            "\n",
            "\n",
            "8. Missing Attribute Values: None\n",
            "\n",
            "\n",
            "9. Class Distribution:\n",
            "\n",
            "\tClass\tExamples\n",
            "\t-----\t--------\n",
            "\t1\t1\n",
            "\t2\t1\n",
            "\t3\t15\n",
            "\t4\t57\n",
            "\t5\t115\n",
            "\t6\t259\n",
            "\t7\t391\n",
            "\t8\t568\n",
            "\t9\t689\n",
            "\t10\t634\n",
            "\t11\t487\n",
            "\t12\t267\n",
            "\t13\t203\n",
            "\t14\t126\n",
            "\t15\t103\n",
            "\t16\t67\n",
            "\t17\t58\n",
            "\t18\t42\n",
            "\t19\t32\n",
            "\t20\t26\n",
            "\t21\t14\n",
            "\t22\t6\n",
            "\t23\t9\n",
            "\t24\t2\n",
            "\t25\t1\n",
            "\t26\t1\n",
            "\t27\t2\n",
            "\t29\t1\n",
            "\t-----\t----\n",
            "\tTotal\t4177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data\n",
        "\n",
        "This section reads the data set and converts it to PyTorch tensors.\n",
        "You probably do not need to modify this section."
      ],
      "metadata": {
        "id": "Pxdc0PRe9ctB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "EKqY2zGfAn7I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abalone_X = []\n",
        "abalone_Y = []\n",
        "with open('abalone.data') as abalone_file:\n",
        "    for line in abalone_file:\n",
        "        line = line.rstrip(\"\\n\")\n",
        "        line = line.split(\",\")\n",
        "\n",
        "        # drop initial sex column\n",
        "        line = line[1:]\n",
        "\n",
        "        # convert from strings to numbers\n",
        "        line = [float(v) for v in line]\n",
        "\n",
        "        abalone_X.append(line[:-1])\n",
        "        abalone_Y.append(line[-1])\n",
        "\n",
        "abalone_X = np.array(abalone_X)\n",
        "abalone_Y = np.array(abalone_Y)"
      ],
      "metadata": {
        "id": "0EliOyJs7n6Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU configuration\n",
        "\n",
        "def to_gpu(t):\n",
        "    if torch.cuda.is_available():\n",
        "        return t.cuda()\n",
        "    return t\n",
        "\n",
        "def to_numpy(t):\n",
        "    return t.detach().cpu().numpy()\n",
        "\n",
        "device = to_gpu(torch.ones(1,1)).device\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W-QTMS7A3w0",
        "outputId": "8c120d80-9cc7-406e-b99a-4f0845542dca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# switch from NumPy arrays to Torch tensors\n",
        "\n",
        "abalone_X = torch.tensor(abalone_X, device=device)\n",
        "abalone_Y = torch.tensor(abalone_Y, device=device)"
      ],
      "metadata": {
        "id": "VAfntp_-AyMn"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abalone_X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQEag8JZBHA9",
        "outputId": "50c55637-9e65-47ca-ff66-47c36eac4470"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4550, 0.3650, 0.0950,  ..., 0.2245, 0.1010, 0.1500],\n",
              "        [0.3500, 0.2650, 0.0900,  ..., 0.0995, 0.0485, 0.0700],\n",
              "        [0.5300, 0.4200, 0.1350,  ..., 0.2565, 0.1415, 0.2100],\n",
              "        ...,\n",
              "        [0.6000, 0.4750, 0.2050,  ..., 0.5255, 0.2875, 0.3080],\n",
              "        [0.6250, 0.4850, 0.1500,  ..., 0.5310, 0.2610, 0.2960],\n",
              "        [0.7100, 0.5550, 0.1950,  ..., 0.9455, 0.3765, 0.4950]],\n",
              "       device='cuda:0', dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abalone_X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P9YXC30BLD6",
        "outputId": "bcbc601f-edcd-415e-9a8a-b59101a16c8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4177, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abalone_Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFNB5RSNBMbJ",
        "outputId": "f25cbe7d-6eed-4853-b300-38fbaf39c279"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([15.,  7.,  9.,  ...,  9., 10., 12.], device='cuda:0',\n",
              "       dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abalone_Y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZHC3Fu1BNuL",
        "outputId": "59219e00-a2f5-4ea7-e9de-0c7257174c74"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4177, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1 - Linear Regression\n",
        "\n",
        "Use PyTorch to implement linear regression of the abalone Rings column saved in `abalone_Y` using the columns in `abalone_X` as inputs.\n",
        "Train your linear model using gradient descent as described in lecture.\n",
        "\n",
        "You can freely use code from the [example training notebook shared in class](https://colab.research.google.com/drive/1xWo_rF0exGdewtaMZP5LNBKJolxUVt8u?usp=sharing).\n",
        "This model should be much simpler than that example - in particular, you do not not need (and should not have) the Fourier features, hidden layers, and activation functions.\n",
        "\n",
        "Feel free to add extra cells as you feel appropriate."
      ],
      "metadata": {
        "id": "SLK6KrMQ87h6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BUILD AND TRAIN YOUR LINEAR MODEL HERE\n",
        "def gradient_descent(X, y, theta, bias,learning_rate, iterations):\n",
        "    m = len(y)\n",
        "    cost_history = torch.zeros(iterations).to(X.device)\n",
        "\n",
        "    for i in range(iterations):\n",
        "        predictions = X.mm(theta)+bias\n",
        "        errors = predictions - y\n",
        "        gradient_theta = (1/m) * X.t().mm(errors)\n",
        "        gradient_b = (1/m) * torch.sum(errors)\n",
        "\n",
        "        theta = theta - learning_rate * gradient_theta\n",
        "        bias = bias - learning_rate * gradient_b\n",
        "\n",
        "\n",
        "        cost_history[i] = (1/(2*m)) * torch.sum(errors**2)\n",
        "\n",
        "    return theta,bias, cost_history\n"
      ],
      "metadata": {
        "id": "gu_4VZNM8_7u"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRINT YOUR LINEAR MODEL COEFFICIENTS HERE\n",
        "abalone_X = abalone_X.to(torch.float32)  # Ensure X is float32\n",
        "abalone_Y = abalone_Y.to(torch.float32)  # Ensure Y is float32\n",
        "theta_initial = torch.randn((abalone_X.shape[1], 1), dtype=torch.float32).to(device)\n",
        "bias = torch.tensor(0.0, dtype=torch.float32, device=device)\n",
        "\n",
        "theta_final, bias, cost_history = gradient_descent(abalone_X, abalone_Y, theta_initial, bias, 0.00001, 1000)\n",
        "\n",
        "print(\"theta：\", theta_final)\n"
      ],
      "metadata": {
        "id": "eCcX2NJY--rS",
        "outputId": "b3c92095-d178-474b-effd-4e520f2d31a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "theta： tensor([[-0.7853],\n",
            "        [-1.4158],\n",
            "        [-0.8171],\n",
            "        [-0.6219],\n",
            "        [ 0.7944],\n",
            "        [ 0.9757],\n",
            "        [-0.6478]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PRINT YOUR LINEAR BIAS HERE.\n",
        "print('bias:' ,bias)\n"
      ],
      "metadata": {
        "id": "_deHI3JA_Fh-",
        "outputId": "fc7200e4-7d63-43d1-8f23-538a91924f49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bias: tensor(0.1127, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abalone_X.shape"
      ],
      "metadata": {
        "id": "tsvGE4jMwwYn",
        "outputId": "df4fde79-20f7-4f3a-a81c-5f849d39eb08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4177, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2 - A Better Model\n",
        "\n",
        "Build and train a separate model using PyTorch using at least one hidden layer.\n",
        "Then answer the questions below.\n",
        "\n",
        "Again, you can freely use code from the [example training notebook shared in class](https://colab.research.google.com/drive/1xWo_rF0exGdewtaMZP5LNBKJolxUVt8u?usp=sharing).\n",
        "You should not need the Fourier features for this particular model.\n",
        "\n",
        "Feel free to add extra cells as you feel appropriate."
      ],
      "metadata": {
        "id": "G96dYHoJ-gDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_width, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_width)\n",
        "        self.fc2 = nn.Linear(hidden_width, hidden_width)\n",
        "        self.fc3 = nn.Linear(hidden_width, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "input_size = 7\n",
        "hidden_width = 128\n",
        "output_size = 1\n",
        "learning_rate = 0.001\n",
        "iterations = 1000\n",
        "\n",
        "x_train = abalone_X\n",
        "y_train = abalone_Y"
      ],
      "metadata": {
        "id": "Q6dRkXl1_h1j"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Describe the second model that you built.\n",
        "Include the number and widths of the hidden layers, the activation functions, and anything else that you deem important."
      ],
      "metadata": {
        "id": "mwi0eFpD_-PZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NN model I use 2 hidden layers and an output layer.Hidden_width =128.Activation function is relu in hidden layers and sigmoid in output layer. I choose the learning rate =0.001 and 1000 iterations."
      ],
      "metadata": {
        "id": "ZyJy2E0JASK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What loss value did your second model achieve?"
      ],
      "metadata": {
        "id": "BWvK1LSDAT51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PRINT YOUR LOSS HERE\n",
        "\n",
        "def train_network(x_train, y_train, input_size, hidden_width, output_size, learning_rate, iterations, device):\n",
        "    x_train = x_train.clone().detach().float().to(device)\n",
        "    y_train = y_train.clone().detach().float().to(device)\n",
        "\n",
        "    model = SimpleNN(input_size=input_size, hidden_width=hidden_width, output_size=output_size).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(iterations):\n",
        "        outputs = model(x_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (epoch+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{iterations}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    return model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "trained_model = train_network(x_train, y_train, input_size, hidden_width, output_size, learning_rate, iterations, device)\n"
      ],
      "metadata": {
        "id": "g-eyHskRAXN5",
        "outputId": "b3933243-2ac8-4b22-909d-e3002dd8647e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/1000], Loss: 90.2152\n",
            "Epoch [200/1000], Loss: 90.2042\n",
            "Epoch [300/1000], Loss: 90.2037\n",
            "Epoch [400/1000], Loss: 90.2036\n",
            "Epoch [500/1000], Loss: 90.2036\n",
            "Epoch [600/1000], Loss: 90.2035\n",
            "Epoch [700/1000], Loss: 90.2035\n",
            "Epoch [800/1000], Loss: 90.2035\n",
            "Epoch [900/1000], Loss: 90.2035\n",
            "Epoch [1000/1000], Loss: 90.2035\n"
          ]
        }
      ]
    }
  ]
}